{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1129278,
          "sourceType": "datasetVersion",
          "datasetId": 635268
        },
        {
          "sourceId": 1130264,
          "sourceType": "datasetVersion",
          "datasetId": 635979
        },
        {
          "sourceId": 1130309,
          "sourceType": "datasetVersion",
          "datasetId": 636012
        },
        {
          "sourceId": 1130383,
          "sourceType": "datasetVersion",
          "datasetId": 636061
        },
        {
          "sourceId": 1130397,
          "sourceType": "datasetVersion",
          "datasetId": 636070
        },
        {
          "sourceId": 1130402,
          "sourceType": "datasetVersion",
          "datasetId": 636074
        },
        {
          "sourceId": 1131250,
          "sourceType": "datasetVersion",
          "datasetId": 636626
        },
        {
          "sourceId": 1131377,
          "sourceType": "datasetVersion",
          "datasetId": 636723
        }
      ],
      "dockerImageVersionId": 29908,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**I am using VGG 19 model pretrained on imagenet for this assignment**"
      ],
      "metadata": {
        "id": "J90bjH4r3BR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is clearly written and is sequential. Most of the steps are self explanatory as they follow the standard template."
      ],
      "metadata": {
        "id": "KMg2eqP43oWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "ikcscGtyxkMU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Importing necessary libraries**"
      ],
      "metadata": {
        "id": "sgOPvWJ_xkMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import keras.preprocessing.image as process_im\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "import functools\n",
        "import IPython.display"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "execution": {
          "iopub.status.busy": "2023-11-26T13:14:11.362186Z",
          "iopub.execute_input": "2023-11-26T13:14:11.362545Z",
          "iopub.status.idle": "2023-11-26T13:14:16.002561Z",
          "shell.execute_reply.started": "2023-11-26T13:14:11.362499Z",
          "shell.execute_reply": "2023-11-26T13:14:16.001660Z"
        },
        "trusted": true,
        "id": "oEr8W7BtxkMY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All you need to do to run on custom image is upload the image and change the paths here (from /content/Dashtoon1.jpeg) to the image paths (style is for the target image style)**"
      ],
      "metadata": {
        "id": "-rgeoQLK0SuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_path='/content/Dashtoon1.jpeg'\n",
        "style_path = '/content/Dashtoon2.jpg'"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "IvxGesqHxkMZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to load images and return numpy array**"
      ],
      "metadata": {
        "id": "P9dIXbmhxkMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(image_path):\n",
        "    image =  Image.open(image_path)\n",
        "    max_dim=512\n",
        "    factor=max_dim/max(image.size)\n",
        "    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n",
        "    im_array = process_im.img_to_array(image)\n",
        "    im_array = np.expand_dims(im_array,axis=0)\n",
        "\n",
        "    return im_array"
      ],
      "metadata": {
        "trusted": true,
        "id": "_MNyn030xkMZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to plot image**"
      ],
      "metadata": {
        "id": "57oxPp20xkMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_im(img,title=None):\n",
        "    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n",
        "    plt.imshow(np.uint8(img))\n",
        "    if title is None:\n",
        "        pass\n",
        "    else:\n",
        "        plt.title(title)\n",
        "    plt.imshow(np.uint8(img))"
      ],
      "metadata": {
        "trusted": true,
        "id": "t4bifrJuxkMa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Image**"
      ],
      "metadata": {
        "id": "vmhlTyyuxkMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = load_file(content_path)\n",
        "style = load_file(style_path)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "htTdjOH1xkMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "content = load_file(content_path)\n",
        "style = load_file(style_path)\n",
        "plt.subplot(1,2,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.subplot(1,2,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "L8LHHlkbxkMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define function to process image for input to vgg19 model**"
      ],
      "metadata": {
        "id": "slq_VFO3xkMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_preprocess(img_path):\n",
        "    image=load_file(img_path)\n",
        "    img=tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return img"
      ],
      "metadata": {
        "trusted": true,
        "id": "MB4H4ot-xkMb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to deprocess image **"
      ],
      "metadata": {
        "id": "XrqXMLtkxkMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG networks are trained on image with each channel normalized by mean = [103.939, 116.779, 123.68]and with channels BGR."
      ],
      "metadata": {
        "id": "P075suC6xkMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deprocess_img(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n",
        "\n",
        "\n",
        "  # perform the inverse of the preprocessing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] # converting BGR to RGB channel\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "QV4Yiu03xkMb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im=img_preprocess(content_path)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94NFMru7xkMb",
        "outputId": "50ff4563-e324-47e9-909a-637cdcd36799"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-eeebf87d3571>:5: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get necessary layers from vgg19 model**"
      ],
      "metadata": {
        "id": "2S0qyqkUxkMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "number_content=len(content_layers)\n",
        "number_style =len(style_layers)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4GOXvaWGxkMb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to get vgg19 model with pretrained weights**"
      ],
      "metadata": {
        "id": "6eyjccOIxkMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "\n",
        "    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "    vgg.trainable=False\n",
        "    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n",
        "    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n",
        "    model_output= style_output+content_output\n",
        "    return models.Model(vgg.input,model_output)"
      ],
      "metadata": {
        "trusted": true,
        "id": "O-UQHkJzxkMc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "model.summary()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8HV53CgxkMc",
        "outputId": "3f31dd4b-3be0-449f-f9de-9a6944826ccd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 3s 0us/step\n",
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20024384 (76.39 MB)\n",
            "Trainable params: 20024384 (76.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture**"
      ],
      "metadata": {
        "id": "9butTtLUxkMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP3DPbqSxkMc",
        "outputId": "44618780-5b7e-4fad-9695-aae92ead5a78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 15,304,768\n",
            "Trainable params: 0\n",
            "Non-trainable params: 15,304,768\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Functions**\n",
        "\n",
        "Neural style transfer is done by defining two loss functions that try to minimise the differences between a content image, a style image and a generated image. Take the base input image, the content image and the style image that needs to be matched and transform the base input image by minimizing the content and style distances (losses) with backpropagation, creating an image that matches the content of the content image and the style of the style image."
      ],
      "metadata": {
        "id": "E4ysWeQ4xkMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**content loss**"
      ],
      "metadata": {
        "id": "dN4-GwGpxkMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_loss(noise,target):\n",
        "    loss = tf.reduce_mean(tf.square(noise-target))\n",
        "    return loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "Un-0MAw3xkMc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **style loss**"
      ],
      "metadata": {
        "id": "XXVy2L_ixkMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(tensor):\n",
        "    channels=int(tensor.shape[-1])\n",
        "    vector=tf.reshape(tensor,[-1,channels])\n",
        "    n=tf.shape(vector)[0]\n",
        "    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n",
        "    return gram_matrix/tf.cast(n,tf.float32)"
      ],
      "metadata": {
        "trusted": true,
        "id": "J9Pg2ZhFxkMd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_loss(noise,target):\n",
        "    gram_noise=gram_matrix(noise)\n",
        "    #gram_target=gram_matrix(target)\n",
        "    loss=tf.reduce_mean(tf.square(target-gram_noise))\n",
        "    return loss\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "TLx2fT9DxkMd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(model,content_path,style_path):\n",
        "    content_img=img_preprocess(content_path)\n",
        "    style_image=img_preprocess(style_path)\n",
        "\n",
        "    content_output=model(content_img)\n",
        "    style_output=model(style_image)\n",
        "\n",
        "    content_feature = [layer[0] for layer in content_output[number_style:]]\n",
        "    style_feature = [layer[0] for layer in style_output[:number_style]]\n",
        "    return content_feature,style_feature\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "_sdy3pHgxkMd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to compute total loss**"
      ],
      "metadata": {
        "id": "T9xgVOFVxkMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n",
        "    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n",
        "                                               #that define what percentage of content and/or style will be preserved in the generated image\n",
        "\n",
        "    output=model(image)\n",
        "    content_loss=0\n",
        "    style_loss=0\n",
        "\n",
        "    noise_style_features = output[:number_style]\n",
        "    noise_content_feature = output[number_style:]\n",
        "\n",
        "    weight_per_layer = 1.0/float(number_style)\n",
        "    for a,b in zip(gram_style_features,noise_style_features):\n",
        "        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n",
        "\n",
        "\n",
        "    weight_per_layer =1.0/ float(number_content)\n",
        "    for a,b in zip(noise_content_feature,content_features):\n",
        "        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n",
        "\n",
        "    style_loss *= style_weight\n",
        "    content_loss *= content_weight\n",
        "\n",
        "    total_loss = content_loss + style_loss\n",
        "\n",
        "\n",
        "    return total_loss,style_loss,content_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "OsqaoDLPxkMd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function to calculate gradient**"
      ],
      "metadata": {
        "id": "35V2phJ5xkMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_grads(dictionary):\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_loss=compute_loss(**dictionary)\n",
        "\n",
        "    total_loss=all_loss[0]\n",
        "    return tape.gradient(total_loss,dictionary['image']),all_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "yvVVfYWHxkMd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "i0S2suzTxkMe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AR1DxlfxkMe",
        "outputId": "a9c88c91-95ae-4957-cd69-971ead05dc88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20024384 (76.39 MB)\n",
            "Trainable params: 20024384 (76.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_style_transfer(content_path,style_path,epochs=500,content_weight=1e3, style_weight=1e-2):\n",
        "\n",
        "    model=get_model()\n",
        "\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    content_feature,style_feature = get_features(model,content_path,style_path)\n",
        "    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n",
        "\n",
        "    noise = img_preprocess(content_path)\n",
        "    noise=tf.Variable(noise,dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    best_loss,best_img=float('inf'),None\n",
        "\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    dictionary={'model':model,\n",
        "              'loss_weights':loss_weights,\n",
        "              'image':noise,\n",
        "              'gram_style_features':style_gram_matrix,\n",
        "              'content_features':content_feature}\n",
        "\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    imgs = []\n",
        "    for i in range(epochs):\n",
        "        grad,all_loss=compute_grads(dictionary)\n",
        "        total_loss,style_loss,content_loss=all_loss\n",
        "        optimizer.apply_gradients([(grad,noise)])\n",
        "        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "        if total_loss<best_loss:\n",
        "            best_loss = total_loss\n",
        "            best_img = deprocess_img(noise.numpy())\n",
        "\n",
        "         #for visualization\n",
        "\n",
        "        if i%5==0:\n",
        "            plot_img = noise.numpy()\n",
        "            plot_img = deprocess_img(plot_img)\n",
        "            imgs.append(plot_img)\n",
        "            IPython.display.clear_output(wait=True)\n",
        "            IPython.display.display_png(Image.fromarray(plot_img))\n",
        "            print('Epoch: {}'.format(i))\n",
        "            print('Total loss: {:.4e}, '\n",
        "              'style loss: {:.4e}, '\n",
        "              'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n",
        "\n",
        "    IPython.display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "    return best_img,best_loss,imgs"
      ],
      "metadata": {
        "trusted": true,
        "id": "hF8KFgDrxkMe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Style Transfer Visualization**"
      ],
      "metadata": {
        "id": "m04NcYHCxkMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best, best_loss,image = run_style_transfer(content_path,\n",
        "                                     style_path, epochs=500)\n"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "ERdCWe6ZxkMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.title('Style transfer Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Y7wXJrstxkMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.title('Style transfer Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "rKHCeBLMxkMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.title('Style transfer Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "YitiD-2txkMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "pjQHLe-rxkMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "1. https://hackernoon.com/how-do-neural-style-transfers-work-7bedaee0559a\n",
        "\n",
        "2. https://arxiv.org/pdf/1701.01036.pdf\n",
        "\n",
        "3. https://towardsdatascience.com/artistic-style-transfer-b7566a216431\n",
        "\n",
        "4. https://arxiv.org/abs/1508.06576\n",
        "\n"
      ],
      "metadata": {
        "id": "XD7S66xXxkMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Thanks**"
      ],
      "metadata": {
        "id": "XYI2Ne5ixkMj"
      }
    }
  ]
}